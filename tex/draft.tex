\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{array}
\bibliographystyle{abbrv}
\usepackage{bibentry}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{geometry} 

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}

\title{Optimal Play for a Non-Ergodic Card Game}
\author{Mees van Dartel, Lorenzo Gregoris}


\begin{document}
\maketitle
\section{Problem formulation}
Consider the following card game \footnote{Of course single player games are not games in the formal sense.}. A decision maker faces a deck $\mathcal{D} = \{x_1, x_2, \dots, x_n\}$ containing $n$ cards, labeled $i \in \{1, 2, \dots, n\}$.
The value of card $x_i=i$. At every time step, the decision maker draws from the deck with uniform probability without replacement.
Denote by $D_t$ the set of remaining cards in the deck at time $t$. The probability of drawing a card $x_i$, conditional on the remaining cards $D_t$, is given by the following PMF:
\begin{equation}
    \mathbb{P}\Big(X=x_i \ \big| \  D_t\Big) =\begin{cases}
        \frac{1}{|D_t|} & \text{for } x_i \in D_t \\
        0               & \text{else}
    \end{cases}.
\end{equation}
Let $x_t$ be the card drawn at time $t$.
The decision maker sequentially draws cards, and faces the following choice. She can \textit{pick} the drawn card $x_t$, and receive reward $r_t=x_t$. However, she must thereafter draw and discard $x_t$ cards from the deck, such that:
\begin{equation}
    D_{t+1} \mid \text{Pick} = D_{t}  \setminus \{y_1, y_2, \ldots, y_{x_t}, x_t\},
\end{equation}
where $y_j$ is drawn uniformly at random without replacement $y_j \sim U(D_t \setminus \{y_1, \dots, y_{j-1}\})$. Alternatively, she can choose to \textit{skip} the card, whereafter she receives reward $r_t=0$, and can draw a new card:
We then have:
\begin{equation}
    D_{t+1} \mid \text{Skip} = D_{t} \setminus \{x_t\}.
\end{equation}
The decision maker's information set at time $t$ is given by $\Omega_t = \{\mathcal{D}, D_t\}$, such that she can observe which cards are left in the deck. At $t=0$, we have $ D_0 = \mathcal{D}$. The decision maker's objective is to maximize her total expected sum of rewards:
\begin{equation}
    R=\mathbb{E}\left[\sum_{t=0}^\infty r_t\right].
\end{equation}
Whenever a time step $\tau$ occurs where there are no cards remaining, such that $D_\tau = \emptyset$, all subsequent rewards are $0$, i.e., $r_t=0$ for all $t \geq \tau$. $D_\tau$ is an absorbing state.
$D_\tau$ eventually occurs with $\mathbb{P}=1$, at the latest at $t=n$, if the decision maker never picks any card. This is clearly suboptimal, as her total sum of rewards will be $R=0$. She can easily improve upon this with the following policy:
\begin{equation}
    \pi(x_t, D_t) = \begin{cases}
        \text{Pick} & \text{if } x_t = n    \\
        \text{Skip} & \text{if } x_t \neq n
    \end{cases},
\end{equation}
such that her total sum of rewards is $R=n$.\\

\textbf{THIS IS A NON-HOMOGENOUS, NON-ERGORIC MARKOV CHAIN? WHAT IS THE STRUCTURE, WHAT DOES THIS SAY ABOUT THE OPTIMAL POLICY?}.

\section{Optimal play}
Consider the following policy:
\begin{equation}
    \pi^{\max}(x_t, D_t) = \begin{cases}
        \text{Pick} & \text{if } x_t = \max\{D_t\}    \\
        \text{Skip} & \text{if } x_t \neq \max\{D_t\}
    \end{cases},
\end{equation}
The policy $\pi^s$ simply searches for the largest card remaining in the deck and ends the game. Once this policy chooses \textit{pick}, we enter the absorbing state $D_\tau$, ending the game. For any state $D_t$, the value of this policy is given by:
\begin{equation}
    V^{\pi^s}(x_t, D_t)= \sum^\infty_t r_t = \max\{D_t\},
\end{equation}
We can think of an example of a deck $D^s$ for which the following policy is optimal, namely a deck for which, for every $x_i \in D^s, x_i > |D^s|$. Picking any card from $D^s$ will generate the absorbing state $D_\tau$, ending the game, so clearly it is optimal to search for the largest card in the deck and end the game.\\

Our proposition for the optimal policy is as follows:
\begin{proposition}
    The optimal policy for the card game is given by:

    \begin{equation}
        \pi^m(x_t, D_t) = \arg \max_{a \in \{\text{Pick, Skip}\}}\Big\{r(x_t,a) + \mathbb{E}\Big[V^{\pi^s}(x_{t+1}, D_{t+1})\Big]\Big\}.
    \end{equation}
\end{proposition}

\begin{proof}
    The Bellman equation for the card game is given by:
    \begin{equation}
        V(x_t, D_t) =  \max_{a \in \{\text{Pick, Skip}\}}\Big\{r(x_t,a) + \mathbb{E}\Big[V(x_{t+1}, D_{t+1}) \ | a \Big]\Big\}.
    \end{equation}
    It follows that if we can show that
    \begin{equation}
        \arg \max_{a \in \{\text{Pick, Skip}\}}\Big\{r(x_t,a) + \mathbb{E}\Big[V(x_{t+1}, D_{t+1})\Big]\Big\} = \arg \max_{a \in \{\text{Pick, Skip}\}}\Big\{r(x_t,a) + \mathbb{E}\Big[V^{\pi^{\max}}(x_{t+1}, D_{t+1})\Big]\Big\},
    \end{equation}
    then  we can be sure that our policy is the optimal policy:
    \begin{equation}
        \pi^m(x_t, D_t) = \pi^*(x_t, D_t).
    \end{equation}
    We can rewrite the RHS as:
    \begin{equation}
        r(x_t,a) + \mathbb{E}\Big[V^{\pi^{\max}}(x_{t+1}, D_{t+1})\Big] = r(x_t,a) + \mathbb{E}\big[\max\{D_{t+1}\}\big].
    \end{equation}
    We can order the elements of $D_t$ as $m_1 > m_2 > \dots > m_{|D_t|}$, where $\max\{D_t\} = m_1$.
    Note that the total number of ways we can discard $x_t$ cards from a deck of size $|D_t|$ is given by
    \begin{equation}
        \binom{|D_t|-1}{x_t}.
    \end{equation}
    We are interested in the number of outcomes for which $\max\{D_t\}=m_j$. These are all events where all cards $m_i>m_j$ are discarded, and $m_j$ is not discarded. The number of events such that these conditions hold is given by
    \begin{equation}
        \binom{|D_t-1|-j}{x_t-(j-1)},
    \end{equation}
    since we can randomize over $x_t-(j-1)$ cards (the cards smaller than $m_j$), and can draw them from $|D_t|-j$ cards. It follows that:
    \begin{align}
         & \mathbb{P}\Big(\max\{D_{T+1}\}  = m_j \big| \text{Pick } x_t\Big) = \mathbb{P}_j  = \frac{\binom{|D_t|-1}{x_t}}{\binom{|D_t|-j-1}{x_t-(j-1)}} .
    \end{align}
    We can then expand the expected maximum after picking $x_t$ as:
    \begin{align}
        \mathbb{E}\big[\max\{D_{t+1}\} \ | \text{Pick} x_t\big] = \sum^{x_t}_{j=1} \mathbb{P}_j\cdot m_j = \sum^{x_t}_{j=1} \frac{\binom{|D_t|-1}{x_t}}{\binom{|D_t|-j-1}{x_t-(j-1)}}  m_j.
    \end{align}
    It follows that the decision makes should select action \textit{Pick} iff:
    \begin{equation}
        x_t+ \sum^{x_t}_{j=1} \frac{\binom{|D_t|-1}{x_t}}{\binom{|D_t|-j-1}{x_t-(j-1)}}  m_j > m_1.
    \end{equation}
    \textbf{WORK IN PROGRESS }$\arrowvert$\\

    We need to show that
    \begin{align}
         & x_t + \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Pick}   \Big] \geq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Skip}   \Big] \implies x_t+  \mathbb{E}\big[\max\{D_{t+1}\} | \text{Pick} \big] \geq \max\{D_t\}
    \end{align}
    and
    \begin{align}
         & x_t + \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Pick}   \Big] \leq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Skip}   \Big] \implies x_t+  \mathbb{E}\big[\max\{D_{t+1}\} | \text{Pick} \big]    \leq \max\{D_t\}   .
    \end{align}


    \begin{lemma}
        Under the optimal policy, any game ending card should be the maximum card left in the deck.
    \end{lemma}
    Consider the first proposition, where the optimal policy would prescribe to Pick. We then have:
    \begin{equation}
        x_t + \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Pick}   \Big] \geq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Skip}   \Big]    \end{equation}

    We know that, by the principle of optimality, the following two inequalities must hold:
    \begin{align}
         & \mathbb{E}\big[\max\{D_{t+1}\} | \text{Pick} \big] \leq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Pick}   \Big], \\
         & \max\{D_t\}  \leq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Skip}   \Big],
    \end{align}
    such that we have
    \begin{equation}
        x_t + \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Pick}   \Big] \geq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Skip}   \Big] \geq \max\{D_t\} .
    \end{equation}
    In the case where $V_\pi^m$ and $V_\pi^*$ are the same, they trivially prescribe the same actions. We are therefore only interested in the case where inequalities are strict:
    \begin{equation}
        x_t + \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Pick}   \Big] \geq \mathbb{E}\Big[V(x_{t+1}, D_{t+1})  |   \text{Skip}   \Big] > \max\{D_t\} .
    \end{equation}

    \textbf{UNFINISHED}.

\end{proof}


\end{document}
